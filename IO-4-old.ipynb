{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TS2 - Problem wieloagentowy - Paweł Malisz, Mikołaj Białek - Rock Paper Scissors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opis gry\n",
    "\n",
    "Papier Kamień Nożyce to jedna z najstarszych dwuosobowych gier ręcznych na świecie. Gracz stara się przewidzieć ruch przeciwnika wnioskując na jego poprzednich ruchach i wygrać za pomocą jednego z elementów, który wybiera na tzn. (raz, dwa, trzy!). W standardowej wersją są trzy elementy: \n",
    " - kamień - wygrywający z nożyczkami,\n",
    " - papier - wygrywający z kamienień,\n",
    " - nożyczki - wygrywające z papierem.\n",
    "\n",
    "Gre można rozszerzyć o dodatkowe elementy takie jak np. jaszczurkę i spock, wtedy mamy wariant 5-elementowy. (zawsze musi być to liczba nieparzysta, czyli 3, 5, 7, 9 itd.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cel ćwiczenia\n",
    "\n",
    "Celem naszego ćwiczenia jest rozwiązanie problemu wieloagentowego z biblioteki PettingZoo używając uczenia mszynowego w języku Python. \n",
    "\n",
    "Przedstawiony zostanie algorytm DQN (Deep Q-learning) na platformie Tianshou wraz z krzywą uczenia."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Opis problemu\n",
    "\n",
    "Na grę składają się dwa dyskretne zbiory akcji i obserwacji. Zbiór akcji zawiera ilość dostępnych ruchów dla gracza (w standardowej wersji są to 3 ruchy). Natomiast zbiór obserwacji składa się z n + 1 ruchów (czyli 4 dla standardowej wersji gry), ostatnią akcją jest tzn. obserwacja, czyli moment w którym gracz jeszcze nic nie wybrał. Chcielibyśmy nauczyć naszego agenta, aby podejmował intuicyjną decyzję bazując na poprzednich ruchach swojego przeciwnika."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Rozwiązanie problemu\n",
    "\n",
    "Zacznijmy od funkcji definiującej nasze środowisko i po krótcę ją opiszmy.\n",
    "\n",
    "Nasze klasyczne środowisko zawiera dwóch agentów oraz poniższe możliwe opcje:\n",
    "- `num_actions` - ilość dostępnych ruchów (elementów) jako liczba nieparzysta,\n",
    "- `max_cycles` - ilośc cykli po jakiej nasi agenci zakończą grę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from pettingzoo.classic import rps_v2\n",
    "\n",
    "def _get_env():\n",
    "    return PettingZooEnv(rps_v2.env(num_actions=3, max_cycles=15))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy nasze środowisko, 100 próbek uczących i 10 testowych, musimy przekształcić na obiekt `DummyVectorEnv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.env import DummyVectorEnv\n",
    "\n",
    "train_envs = DummyVectorEnv([_get_env for _ in range(100)])\n",
    "test_envs = DummyVectorEnv([_get_env for _ in range(10)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz stwórzmy funkcję dla naszych agentów, zrobimy tak, by `player_1` był naszym uczeniem maszynowym, a `player_0` podejmował losowe ruchy.\n",
    "\n",
    "Opis kodu:\n",
    "- `player_0` = `agent_opponent` - struktura klasy `RandomPolicy()`,\n",
    "- `player_1` = `agent_learn` - struktura klasy `DQNPolicy()`, która przyjmuje takie wartości jak:\n",
    "    - `model` - struktura w tym przypadku klasy `Net()`, (chociaż mogłaby być to też zapewne `Actor()` z `tianshou.utils.net.discrete`), opisuje nam stosowaną sieć neuronową, domyślnie z funkcją aktywacji ReLu,\n",
    "    - `optim` - czyli optymalizator, domnyslnie sotuje się Adam, więc taki zostawiamy,\n",
    "    - `discount_factor`,\n",
    "    - `estimation_step` - przewidywana ilość ruchów (w naszym przypadku 1),\n",
    "    - `target_update_freq` - określa częstotliwość aktualizacji tzw. target network (sieci docelowej) w stosunku do policy network (sieci polityki).\n",
    "\n",
    "Później łączymy dwie polityki w jedną `MultiAgentPolicyManager()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy, PPOPolicy\n",
    "from tianshou.utils.net.common import Net, ActorCritic\n",
    "from tianshou.utils.net.discrete import Actor, Critic\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = _get_env()\n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        net = Net(\n",
    "            state_shape=env.observation_space.shape or env.observation_space.n,\n",
    "            action_shape=env.action_space.shape or env.action_space.n,\n",
    "            hidden_sizes=[64, 64],\n",
    "            device=device,\n",
    "        ).to(device)\n",
    "        actor = Actor(net, env.action_space.n, device=device).to(device)\n",
    "        critic = Critic(net, device=device).to(device)\n",
    "        actor_critic = ActorCritic(actor, critic)\n",
    "\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(actor_critic.parameters(), lr=0.0003)\n",
    "            dist = torch.distributions.Categorical\n",
    "            agent_learn = PPOPolicy(actor, critic, optim, dist, action_space=env.action_space.shape or env.action_space.n, deterministic_eval=True)\n",
    "\n",
    "    if agent_opponent is None:\n",
    "        agent_opponent = RandomPolicy()\n",
    "\n",
    "    agents = [agent_opponent, agent_learn]\n",
    "    print(agents)\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie inicjalizujemy naszych agentów, oraz ustawiamy Collectory, czyli obiekt `Collector` z biblioteki `Tianshou`. Służy onm do zbierania danych treningowych oraz testowych w procesie uczenia ze wzmocnieniem. Służy do interakcji z środowiskiem, gromadzenia trajektorii agentów oraz przechowywania tych trajektorii w pamięci podręcznej w celu późniejszego wykorzystania w procesie uczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "\n",
    "policy, optim, agents = _get_agents()\n",
    "policy.train()\n",
    "\n",
    "train_collector = Collector(policy, train_envs, VectorReplayBuffer(50000, len(train_envs)))\n",
    "test_collector = Collector(policy, test_envs)\n",
    "\n",
    "train_collector.reset()\n",
    "train_collector.collect(n_episode=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz etap uczenia, służy do niego funkcja `offpolicy_trainer()` z biblioteki `tianshou`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.trainer import offpolicy_trainer\n",
    "import tianshou as ts\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "lr, epoch, batch_size = 1e-3, 10, 64\n",
    "train_num, test_num = 10, 100\n",
    "gamma, n_step, target_freq = 0.9, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.1, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, 10\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))\n",
    "\n",
    "def save_best_fn(policy):\n",
    "    model_save_path = os.path.join(\"log\", \"rps\", \"dqn\", \"policy.pth\")\n",
    "    os.makedirs(os.path.join(\"log\", \"rps\", \"dqn\"), exist_ok=True)\n",
    "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= 0.6\n",
    "\n",
    "def train_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.1)\n",
    "\n",
    "def test_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.05)\n",
    "\n",
    "def reward_metric(rews):\n",
    "    return rews[:, 1]\n",
    "\n",
    "result = offpolicy_trainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=50,\n",
    "    step_per_epoch=1000,\n",
    "    step_per_collect=50,\n",
    "    episode_per_test=10,\n",
    "    batch_size=64,\n",
    "    train_fn=train_fn,\n",
    "    test_fn=test_fn,\n",
    "    stop_fn=stop_fn,\n",
    "    save_best_fn=save_best_fn,\n",
    "    update_per_step=0.1,\n",
    "    test_in_train=False,\n",
    "    reward_metric=reward_metric,\n",
    "    logger=logger\n",
    ")\n",
    "print(f'Finished training! Use {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.trainer import onpolicy_trainer\n",
    "\n",
    "result = onpolicy_trainer(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=50000,\n",
    "    repeat_per_collect=10,\n",
    "    episode_per_test=10,\n",
    "    batch_size=256,\n",
    "    step_per_collect=2000,\n",
    "    stop_fn=lambda mean_reward: mean_reward >= 195,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapisanie wytrenowanej polityki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wytestowanie naszego uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "collector = ts.data.Collector(policy, _get_env(), exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1 / 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
