{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TS2 - Problem wieloagentowy - Paweł Malisz, Mikołaj Białek - Tic-Tac-Toe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opis gry\n",
    "\n",
    "Kółko i krzyżyk to jedna z najstarszych dwuosobowych gier ręcznych na świecie. Gracze starają się ułożyć przypisany sobie znak (X lub O) w jednym rzędzie, kolumnie bądź skosie. Wygrywa ten, który połączy w ten sposób trzy znaki (w tradycyjnej odmianie)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cel ćwiczenia\n",
    "\n",
    "Celem naszego ćwiczenia jest rozwiązanie problemu wieloagentowego z biblioteki PettingZoo używając uczenia mszynowego w języku Python. \n",
    "\n",
    "Przedstawiony zostanie algorytm DQN (Deep Q-learning) na platformie Tianshou wraz z krzywą uczenia."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Opis problemu\n",
    "\n",
    "Na grę składają się dwa ciągłe zbiory akcji i obserwacji. Zbiór akcji zawiera pojedyńczą liczbę oznaczającą ruch gracza - jest to liczba od 0-8, który symbolizuję numer pola. \n",
    "\n",
    "0 | 3 | 6\n",
    "\n",
    "1 | 4 | 7\n",
    "\n",
    "2 | 5 | 8\n",
    "\n",
    "Natomiast zbiór obserwacji składa się z siatki 3x3 oraz dwóch wymiarów, przy czym pierwszy oznacza rozmienieczenie X, a drugi rozmieszczenie O. Obydwa wymiary zawierają zbiór 0 i 1. W pierwszym 1 jest w momencie, gdy występuje na niej X, natomst 0, gdy tego X tam nie ma. Analogicznie w drugim wymiarze dla O."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Rozwiązanie problemu\n",
    "\n",
    "Zacznijmy od funkcji definiującej nasze środowisko i po krótcę ją opiszmy. Nasze klasyczne środowisko zawiera dwóch agentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "def _get_env():\n",
    "    return PettingZooEnv(tictactoe_v3.env())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy nasze środowisko, 100 próbek uczących i 100 testowych, musimy przekształcić na obiekt `DummyVectorEnv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.env import DummyVectorEnv\n",
    "\n",
    "train_envs = DummyVectorEnv([_get_env for _ in range(100)])\n",
    "test_envs = DummyVectorEnv([_get_env for _ in range(100)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz stwórzmy funkcję dla naszych agentów, zrobimy tak, by `player_1` był naszym uczeniem maszynowym, a `player_0` podejmował losowe ruchy.\n",
    "\n",
    "Opis kodu:\n",
    "- `player_0` = `agent_opponent` - struktura klasy `RandomPolicy()`,\n",
    "- `player_1` = `agent_learn` - struktura klasy `DQNPolicy()`, która przyjmuje takie wartości jak:\n",
    "    - `model` - struktura w tym przypadku klasy `Net()`, opisuje nam stosowaną sieć neuronową, domyślnie z funkcją aktywacji ReLu, która zawiera kształt obserwacji na wejściu, kształt akcji na wyjściu, 4 warstwy ukryte po 128 neuronów oraz device, który wykorzystuje rdzenie cuda karty graficznej, jeśli te są dostępne,\n",
    "    - `optim` - czyli optymalizator, domnyslnie sotuje się Adam, więc taki zostawiamy,\n",
    "    - `discount_factor` - stosowany w AI, im większy tym teoretycznie lepiej,\n",
    "    - `estimation_step` - przewidywana ilość ruchów (w naszym przypadku 3 - potrzebne by szybko wygrać),\n",
    "    - `target_update_freq` - określa częstotliwość aktualizacji tzw. target network (sieci docelowej) w stosunku do policy network (sieci polityki).\n",
    "\n",
    "Później łączymy dwie polityki w jedną `MultiAgentPolicyManager()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import gymnasium as gym\n",
    "from typing import Optional, Tuple\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy, PPOPolicy\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = _get_env()\n",
    "    observation_space = (\n",
    "        env.observation_space[\"observation\"]\n",
    "        if isinstance(env.observation_space, gym.spaces.Dict)\n",
    "        else env.observation_space\n",
    "    )\n",
    "    state_shape = observation_space.shape or observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    if agent_learn is None:\n",
    "        net = Net(\n",
    "            state_shape = state_shape,\n",
    "            action_shape = action_shape,\n",
    "            hidden_sizes = [128, 128, 128, 128],\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "        agent_learn = DQNPolicy(\n",
    "            model = net,\n",
    "            optim = optim,\n",
    "            discount_factor = 0.9,\n",
    "            estimation_step = 3,\n",
    "            target_update_freq = 320,\n",
    "        )\n",
    "\n",
    "    if agent_opponent is None:\n",
    "        agent_opponent = RandomPolicy()\n",
    "\n",
    "    agents = [agent_opponent, agent_learn]\n",
    "    print(agents)\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie inicjalizujemy naszych agentów, oraz ustawiamy Collectory, czyli obiekt `Collector` z biblioteki `Tianshou`. Służy onm do zbierania danych treningowych oraz testowych w procesie uczenia ze wzmocnieniem. Służy do interakcji z środowiskiem, gromadzenia trajektorii agentów oraz przechowywania tych trajektorii w pamięci podręcznej w celu późniejszego wykorzystania w procesie uczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "\n",
    "policy, optim, agents = _get_agents()\n",
    "policy.train()\n",
    "\n",
    "train_collector = Collector(policy, train_envs, VectorReplayBuffer(20000, len(train_envs)))\n",
    "test_collector = Collector(policy, test_envs)\n",
    "\n",
    "train_collector.reset()\n",
    "train_collector.collect(n_step=len(train_envs)*64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz etap uczenia, służy do niego funkcja `offpolicy_trainer()` z biblioteki `tianshou`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.trainer import offpolicy_trainer\n",
    "import tianshou as ts\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))\n",
    "\n",
    "def save_best_fn(policy):\n",
    "    model_save_path = os.path.join(\"log\", \"ttt\", \"dqn\", \"policy.pth\")\n",
    "    os.makedirs(os.path.join(\"log\", \"ttt\", \"dqn\"), exist_ok=True)\n",
    "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= 0.6 # end, when the win rate is equal or greater than 0.6\n",
    "\n",
    "def train_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.1)\n",
    "\n",
    "def test_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.05)\n",
    "\n",
    "def reward_metric(rews):\n",
    "    return rews[:, 1]\n",
    "\n",
    "result = offpolicy_trainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=50,\n",
    "    step_per_epoch=1000,\n",
    "    step_per_collect=10,\n",
    "    episode_per_test=100,\n",
    "    batch_size=64,\n",
    "    train_fn=train_fn,\n",
    "    test_fn=test_fn,\n",
    "    stop_fn=stop_fn,\n",
    "    save_best_fn=save_best_fn,\n",
    "    update_per_step=0.1,\n",
    "    test_in_train=False,\n",
    "    reward_metric=reward_metric,\n",
    "    logger=logger\n",
    ")\n",
    "print(f'Finished training! Use {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby zobaczyć krzywe uczenia, musimy odpalić nasze logi za pomocą poniższej komendy w terminalu:\n",
    "\n",
    "`tensorboard --logdir log`\n",
    "\n",
    "![alt text](images/train.png \"Wykres na zbiorze uczącym.\")\n",
    "\n",
    "![alt text](images/test.png \"Wykres na zbiorze testowym.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wytestowanie naszego uczenia na jednej przykładowej grze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "my_env = PettingZooEnv(tictactoe_v3.env(render_mode=\"human\"))\n",
    "my_env = DummyVectorEnv([lambda: my_env])\n",
    "collector = Collector(policy, my_env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
